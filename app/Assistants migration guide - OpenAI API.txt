Assistants
migration guide

Copy page

What's changed?
Assistants to prompts

Migrate from the Assistants
API to the Responses API.

Threads to conversations
Runs to responses
Migrating your integration

After achieving feature parity in the Responses
API, we've deprecated the Assistants API. It will
shut down on August 26, 2026. Follow the
guidance below to update your integration.
Learn more.

We're moving from the Assistants API to the new
Responses API for a simpler and more flexible
mental model.
Responses are simpler—send input items and get
output items back. With the Responses API, you
also get better performance and new features like
deep research, MCP, and computer use. This
change also lets you manage conversations instead
of passing back previous_response_id .

What's changed?
BEFORE

NOW

WHY?

Assistants

Prompts

Prompts hold
configuration (model,
tools, instructions) and
are easier to version and
update

Threads

Conversations

Streams of items instead

Comparing full examples

of just messages
Runs

Responses

Responses send input
items or use a
conversation object and
receive output items; tool
call loops are explicitly
managed

Run steps

Items

Generalized objects—can
be messages, tool calls,
outputs, and more

From assistants to prompts
Assistants were persistent API objects that bundled
model choice, instructions, and tool declarations—
created and managed entirely through the API.
Their replacement, prompts, can only be created in
the dashboard, where you can version them as you
develop your product.

Why this is helpful
Portability and versioning: You can snapshot,
review, diff, and roll back prompt specs. You can
also version a prompt, so your code can just
point the latest version.
Separation of concerns: Your application code
now handles orchestration (history pruning,
tool loop, retries) while your prompt focuses on
high-level behavior and constraints (system
guidance, tool availability, structured output
schema, temperature defaults).
Realtime compatibility: The same prompt
configuration can be reused when you connect
through the Realtime API, giving you a single
definition of behavior across chat, streaming,

and low-latency interactive sessions.
Tool and output consistency: Using prompts,
every Responses or Realtime session you start
inherits a consistent contract because prompts
encapsulate tool schemas and structured
output expectations.

Practical migration steps
1

Identify each existing Assistant’s instruction +
tool bundle.

2

In the dashboard, recreate that bundle as a
named prompt.

3

Store the prompt ID (or its exported spec) in
source control so application code can refer to
a stable identifier.

4

During rollout, run A/B tests by swapping
prompt IDs—no need to create or delete
assistant objects programmatically.

Think of a prompt as a versioned behavioral profile
to plug into either Responses or Realtime API.

From threads to
conversations
A thread was a collection of messages stored
server-side. Threads could only store messages.
Conversations store items, which can include
messages, tool calls, tool outputs, and other data.

Request example

Thread object

Conversation object

1 thread = openai.beta.threads.create(
1 conversation = openai.conversations.create(
2
messages=[{"role": "user",
2
items=[{"role":
"content": "what
"user",
are "content":
the 5 Ds of"what
dodgeball?"}],
are the 5
3
metadata={"user_id":3 "peter_le_fleur"},
metadata={"user_id": "peter_le_fleur"},
4 )
4 )

Response example
Thread object

1
2
3
4
5
6
7
8
9

Conversation object

{
1 {
"id": "thread_CrXtCzcyEQbkAcXuNmVSKFs1",
2 "id": "conv_68542dc602388199a30af27d040cefd4087a04b57
"object": "thread",
3 "object": "conversation",
"created_at": 1752855924,
4 "created_at": 1752855924,
"metadata": {
5 "metadata": {
"user_id": "peter_le_fleur"
6
user_id": "peter_le_fleur"
},
7 }
"tool_resources": {} 8 }
}

From runs to responses
Runs were asynchronous processes that executed
against threads. See the example below. Responses
are simpler: provide a set of input items to execute,
and get a list of output items back.
Responses are designed to be used alone, but you
can also use them with prompt and conversation
objects for storing context and configuration.

Request example

Run object

1
2
3
4
5
6
7
8

Response object

thread_id = "thread_CrXtCzcyEQbkAcXuNmVSKFs1"
1 response = openai.responses.create(
assistant_id = "asst_8fVY45hU3IM6creFkVi5MBKB"
2
model="gpt-4.1",
3
input=[{"role": "user", "content": "What are the 5
run = openai.beta.threads.runs.create(thread_id=thread_id,
4
conversation: "conv_689667905b048191b4740501625afd9
assistant_id=assis
5 )
while run.status in ("queued", "in_progress"):
time.sleep(1)
run = openai.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)

Response example
Run object

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

Response object

{
1 {
"id": "run_FKIpcs5ECSwuCmehBqsqkORj",
2 "id": "resp_687a7b53036c819baad6012d58b39bcb074adcd9
"assistant_id": "asst_8fVY45hU3IM6creFkVi5MBKB",
3 "created_at": 1752857427,
"cancelled_at": null,4 "conversation": {
"completed_at": 1752857327,
5
"id": "conv_689667905b048191b4740501625afd940c7533
"created_at": 1752857322,
6 },
"expires_at": null, 7 "error": null,
"failed_at": null,
8 "incomplete_details": null,
"incomplete_details":9 null,
"instructions": null,
"instructions": null,10 "metadata": {},
"last_error": null, 11 "model": "gpt-4.1-2025-04-14",
"max_completion_tokens":
12 "object":
null,
"response",
"max_prompt_tokens": 13
null,
"output": [
"metadata": {},
14
{
"model": "gpt-4.1", 15
"id": "msg_687a7b542948819ba79e77e14791ef83074ad
"object": "thread.run",
16
"content": [
"parallel_tool_calls":
17 true, {
"required_action": null,
18
"annotations": [],
"response_format": "auto",
19
"text": "The \"5 Ds of Dodgeball\" are a hum
"started_at": 1752857324,
20
"status": "completed",
21 1. **Dodge**
"thread_id": "thread_CrXtCzcyEQbkAcXuNmVSKFs1",
22 2. **Duck**
"tool_choice": "auto",
23 3. **Dip**
"tools": [],
24 4. **Dive**
"truncation_strategy":
25 {5. **Dodge** (yes, dodge is listed twice for emphasi
"type": "auto",
26

27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44

"last_messages": null
27 In summary:
},
28 > **“If you can dodge a wrench, you can dodge a ball
"usage": {
29
"completion_tokens":
30 130,
These 5 Ds are not official competitive rules, but h
"prompt_tokens": 34,
31
"type": "output_text",
"total_tokens": 164,
32
"logprobs": []
"prompt_token_details":
33
{
}
"cached_tokens": 34
0
],
},
35
"role": "assistant",
"completion_tokens_details":
36
"status":
{
"completed",
"reasoning_tokens":
37 0
"type": "message"
}
38
}
},
39 ],
"temperature": 1.0, 40 "parallel_tool_calls": true,
"top_p": 1.0,
41 "temperature": 1.0,
"tool_resources": {},42 "tool_choice": "auto",
"reasoning_effort": null
43 "tools": [],
}
44 "top_p": 1.0,
45 "background": false,
46 "max_output_tokens": null,
47 "previous_response_id": null,
48 "reasoning": {
49
"effort": null,
50
"generate_summary": null,
51
"summary": null
52 },
53 "service_tier": "scale",
54 "status": "completed",
55 "text": {
56
"format": {
57
"type": "text"
58
}
59 },
60 "truncation": "disabled",
61 "usage": {
62
"input_tokens": 17,
63
"input_tokens_details": {
64
"cached_tokens": 0
65
},
66
"output_tokens": 150,
67
"output_tokens_details": {
68
"reasoning_tokens": 0
69
},
70
"total_tokens": 167
71 },

72
73
74
75
76

"user": null,
"max_tool_calls": null,
"store": true,
"top_logprobs": 0
}

Migrating your integration
Follow the migration steps below to move from the
Assistants API to the Responses API, without losing
any feature support.

1. Create prompts from your
assistants
1

Identify the most important assistant objects in
your application.

2

Find these in the dashboard and click
Create prompt .

This will create a prompt object out of each existing
assistant object.

2. Move new user chats over to
conversations and responses
We will not provide an automated tool for migrating
Threads to Conversations. Instead, we recommend
migrating new user threads onto conversations and
backfilling old ones as necessary.
Here's an example for how you might backfill a
thread:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

thread_id = "thread_EIpHrTAVe0OzoLQg3TXfvrkG"

for page in openai.beta.threads.messages.list(thread_id=thread_id, order="as
messages += page.data
items = []
for m in messages:
item = {"role": m.role}
item_content = []

for content in m.content:
match content.type:
case "text":
item_content_type = "input_text" if m.role == "user" else "o
item_content += [{"type": item_content_type, "text": content
case "image_url":
item_content + [
{
"type": "input_image",
"image_url": content.image_url.url,
"detail": content.image_url.detail,
}
]
item |= {"content": item_content}
items.append(item)
# create a conversation with your converted items
conversation = openai.conversations.create(items=items)

Comparing full examples
Here’s a few simple examples of integrations using
both the Assistants API and the Responses API so
you can see how they compare.

User chat app
Assistants API

Responses API

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

thread = openai.threads.create()
@app.post("/messages")
async def message(message: Message):
openai.beta.threads.messages.create(
role="user",
content=message.content
)

run = openai.beta.threads.runs.create(
assistant_id=os.getenv("ASSISTANT_ID"),
thread_id=thread.id
)
while run.status in ("queued", "in_progress"):
await asyncio.sleep(1)
run = openai.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id
messages = openai.beta.threads.messages.list(
order="desc", limit=1, thread_id=thread.id
)
return { "content": messages[-1].content }

