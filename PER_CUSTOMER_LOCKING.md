# Per-Customer Linear Conversation Processing

**Date**: November 7, 2025  
**Requirement**: Linear message processing per customer, concurrent processing across customers  
**Implementation**: Database-based processing locks  
**Status**: ‚úÖ IMPLEMENTED

---

## üéØ **The Requirement**

### **User's Exact Words**:
> "Each conversation must be handled in a linear fashion. If we're already processing a customer message and another message from that same customer arrives, the message must be buffered until the current message sent to OpenAI is answered. On the other hand if the message comes from another customer, then there's no issue in having another worker process the conversation."

### **Translation to Architecture**:
```
‚úÖ Customer A (Message 1) ‚Üí Worker 1 ‚Üí Processing
‚ùå Customer A (Message 2) ‚Üí Worker 2 ‚Üí MUST WAIT (not start new processing)
‚úÖ Customer B (Message 1) ‚Üí Worker 2 ‚Üí Processing (concurrent with A)
‚úÖ Customer C (Message 1) ‚Üí Worker 3 ‚Üí Processing (concurrent with A & B)
```

---

## üî¥ **The Problem with Multiple Workers**

### **Without Coordination (BROKEN)**:
```python
# Worker 1 memory:
waid_timers = {"50376819621": <thread>}  # Processing Customer A

# Worker 2 memory:
waid_timers = {}  # Doesn't know Worker 1 is processing!
```

**Scenario**:
```
00:00 - Customer A sends "Hola"
00:00 - Webhook arrives ‚Üí Worker 1 ‚Üí Starts timer
00:05 - Customer A sends "Quiero reservar"
00:05 - Webhook arrives ‚Üí Worker 2 ‚Üí Checks local dict ‚Üí No timer found
00:05 - Worker 2 starts ANOTHER timer ‚ùå
00:65 - Worker 1 sends response based on "Hola"
00:70 - Worker 2 sends response based on "Hola" + "Quiero reservar"
Result: TWO responses, conversation out of order!
```

---

## ‚úÖ **The Solution: Database-Based Locks**

### **1. Processing Lock Table**

**Schema** (`message_buffer.py` lines 75-85):
```sql
CREATE TABLE processing_lock (
    wa_id TEXT PRIMARY KEY,        -- Only one lock per customer
    locked_at DATETIME,            -- When lock was acquired
    worker_pid INTEGER             -- Which worker holds it
)
```

### **2. Lock Acquisition** (`message_buffer.py` lines 168-200)

```python
def try_acquire_processing_lock(wa_id: str) -> bool:
    """Try to acquire lock for customer conversation.
    
    Returns True if acquired (this worker can process).
    Returns False if locked by another worker.
    """
    worker_pid = os.getpid()
    
    try:
        # Try INSERT (will fail if PRIMARY KEY exists)
        conn.execute(
            "INSERT INTO processing_lock VALUES (?, CURRENT_TIMESTAMP, ?)",
            (wa_id, worker_pid)
        )
        return True  # Got the lock!
    except sqlite3.IntegrityError:
        return False  # Another worker has the lock
```

**How PRIMARY KEY Prevents Race**:
- SQLite PRIMARY KEY is atomic
- Only ONE insert can succeed
- All others get IntegrityError
- No race condition possible

### **3. Webhook Handler** (`main.py` lines 1582-1599)

```python
# Buffer message first (always)
message_buffer.buffer_message(phone_number, message_type, content, ...)

# Try to acquire lock
lock_acquired = message_buffer.try_acquire_processing_lock(phone_number)

if lock_acquired:
    # We got the lock - start timer
    start_timer(phone_number)
else:
    # Another worker has the lock - just buffer
    logger.info(f"Another worker processing {phone_number}, message buffered")
    # Message will be included in that worker's batch
```

### **4. Lock Release** (`main.py` line 1078)

```python
# After timer finishes processing
message_buffer.release_processing_lock(wa_id)
# Now other workers can process next message from this customer
```

---

## üìä **How It Works End-to-End**

### **Scenario: Rapid Messages from Same Customer**

```
Time    Event                           Worker    Action
----------------------------------------------------------------------
00:00   Customer A: "Hola"             
        ‚Üí Webhook arrives               Worker 1   Buffers message
        ‚Üí Try acquire lock              Worker 1   ‚úÖ SUCCESS (gets lock)
        ‚Üí Start timer                   Worker 1   Timer counting (65s)

00:05   Customer A: "Quiero reservar"
        ‚Üí Webhook arrives               Worker 2   Buffers message
        ‚Üí Try acquire lock              Worker 2   ‚ùå FAILS (Worker 1 has it)
        ‚Üí Just buffer                   Worker 2   Returns immediately

01:05   Timer fires                     Worker 1   Gets BOTH messages from buffer
        ‚Üí Process "Hola\nQuiero..."     Worker 1   Sends to OpenAI
        ‚Üí Get response                  Worker 1   Single coherent response
        ‚Üí Send to customer              Worker 1   Customer gets ONE message
        ‚Üí Release lock                  Worker 1   Lock deleted from DB

01:06   Customer A: "Gracias"
        ‚Üí Webhook arrives               Worker 3   Buffers message
        ‚Üí Try acquire lock              Worker 3   ‚úÖ SUCCESS (lock was released)
        ‚Üí Start timer                   Worker 3   New timer starts
```

### **Scenario: Multiple Customers Concurrently**

```
Time    Event                           Worker    Action
----------------------------------------------------------------------
00:00   Customer A: "Hola"             Worker 1   Acquires lock for A ‚úÖ
00:00   Customer B: "Buenos d√≠as"      Worker 2   Acquires lock for B ‚úÖ
00:00   Customer C: "Informaci√≥n"      Worker 3   Acquires lock for C ‚úÖ
00:00   Customer D: "Reserva"          Worker 4   Acquires lock for D ‚úÖ

All 4 workers processing DIFFERENT customers in parallel ‚úÖ
```

---

## üõ°Ô∏è **Safety Mechanisms**

### **1. Stale Lock Cleanup** (`message_buffer.py` lines 215-237)

**Problem**: Worker crashes while holding lock ‚Üí lock never released ‚Üí customer stuck forever

**Solution**: Cleanup on startup
```python
def cleanup_stale_locks(max_age_minutes=10):
    """Remove locks older than 10 minutes (worker probably crashed)"""
    cutoff = datetime.utcnow() - timedelta(minutes=10)
    conn.execute("DELETE FROM processing_lock WHERE locked_at < ?", cutoff)
```

Called on service startup (`main.py` lines 709-713).

### **2. Orphaned Message Detection**

Already implemented (from previous fix). If lock cleanup removes a stale lock, orphaned messages will be detected and processed.

### **3. Message Buffer Coordination**

Even if multiple timers start (shouldn't happen, but defensive):
- Both timers call `get_and_clear_buffered_messages()`
- First one gets messages and DELETES them
- Second one finds nothing and exits cleanly
- No duplicate processing

---

## üß™ **Testing Scenarios**

### **Test 1: Rapid Same-Customer Messages**

```bash
# Send 3 messages from same customer with 2-second gaps
for i in 1 2 3; do
  curl -X POST http://127.0.0.1:8006/webhook \
    -H 'Content-Type: application/json' \
    -d "{\"waId\":\"50376819621\",\"text\":\"Message $i\",\"passkey\":\"FuK@tTcKerZ-2o25\"}"
  sleep 2
done
```

**Expected behavior**:
```bash
journalctl -u watibot4 -f | grep "50376819621"

# Should see:
# - "Acquired lock and started timer" (once)
# - "Another worker processing" (twice)
# - One combined OpenAI call with all 3 messages
# - One response sent to customer
```

### **Test 2: Multiple Customers Concurrently**

```bash
# Send 4 messages from different customers simultaneously
for i in 1 2 3 4; do
  curl -X POST http://127.0.0.1:8006/webhook \
    -d "{\"waId\":\"5037681962$i\",\"text\":\"Test\",\"passkey\":\"FuK@tTcKerZ-2o25\"}" &
done
wait
```

**Expected behavior**:
```bash
# All 4 should show "Acquired lock and started timer"
# No "Another worker processing" messages
# 4 separate OpenAI calls
# 4 separate responses
```

### **Test 3: Stale Lock Recovery**

```bash
# Manually insert a stale lock
sqlite3 thread_store.db "INSERT INTO processing_lock VALUES ('50399999999', datetime('now', '-15 minutes'), 99999)"

# Restart service
sudo systemctl restart watibot4

# Check logs
journalctl -u watibot4 -n 50 | grep "stale"
# Should see: "Cleaned up 1 stale processing locks"
```

---

## üìà **Performance Characteristics**

### **Lock Acquisition Time**
- SQLite INSERT: < 1ms typically
- PRIMARY KEY check: Atomic, no race
- **Total webhook delay**: < 2ms additional

### **Concurrency**
- **Same customer**: Serialized (as required)
- **Different customers**: Fully concurrent
- **Capacity**: 4 workers √ó ~20 customers = ~80 concurrent conversations

### **Resource Usage**
- Processing lock table: Minimal (1 row per active conversation)
- Max rows: ~80 (if all workers busy)
- Disk space: ~5KB typical

---

## üîç **Monitoring & Debugging**

### **Check Active Locks**

```bash
sqlite3 thread_store.db "SELECT wa_id, worker_pid, locked_at FROM processing_lock"
```

**Healthy state**: 0-4 locks (one per busy worker)  
**Problem**: > 4 locks (stale locks or excessive concurrency)

### **Log Patterns**

**Normal**:
```
[LOCK] Acquired processing lock for 50376819621 (worker PID 12345)
[TIMER_THREAD] Acquired lock and started timer for 50376819621
... 65 seconds later ...
[LOCK] Released processing lock for 50376819621 (worker PID 12345)
```

**Coordination working**:
```
[LOCK] Processing lock for 50376819621 already held by worker PID 12345
[TIMER_THREAD] Another worker processing 50376819621, message buffered
```

**Stale lock cleanup**:
```
[LOCK] Found 2 stale locks, cleaning up: [('50376819621', 12345, '2025-11-07 00:30:00'), ...]
```

### **Alert Conditions**

- ‚ùå Lock held > 5 minutes ‚Üí Worker may be stuck
- ‚ùå > 10 stale locks on startup ‚Üí Frequent crashes
- ‚ùå "Already held" logs > 50% ‚Üí May need more workers

---

## üéØ **Key Design Decisions**

### **Why Database Lock vs Redis/Memcached?**

**Pros of SQLite**:
- ‚úÖ Already using SQLite for message buffer
- ‚úÖ ACID guarantees (atomic PRIMARY KEY)
- ‚úÖ No additional infrastructure
- ‚úÖ Survives restarts (can detect stale locks)

**Cons**:
- ‚ö†Ô∏è Write serialization (not an issue at our scale)
- ‚ö†Ô∏è Single file (but we're using multiple workers on same machine)

**Redis would be better if**:
- Running across multiple servers
- Need > 100 workers
- Need TTL on locks (auto-expire)

### **Why Lock on Timer Start vs Message Arrival?**

**Current**: Lock acquired AFTER buffering message

**Alternative**: Lock acquired BEFORE buffering

**Rationale**:
- Messages must always be buffered (never drop)
- Buffering is fast (< 5ms)
- Lock only prevents timer start
- If lock fails, message still buffered and will be included in active batch

---

## üìö **Files Modified**

1. **`/home/robin/watibot4/app/message_buffer.py`**
   - Lines 75-85: Create processing_lock table
   - Lines 168-200: `try_acquire_processing_lock()`
   - Lines 202-213: `release_processing_lock()`
   - Lines 215-237: `cleanup_stale_locks()`

2. **`/home/robin/watibot4/app/main.py`**
   - Lines 709-713: Stale lock cleanup on startup
   - Lines 1582-1599: Lock-based timer start in webhook
   - Line 1078: Lock release after timer completes

3. **`/home/robin/watibot4/start_watibot4.sh`**
   - Line 53: `--workers 4` flag

---

## ‚úÖ **Validation Checklist**

Before deploying:
- [ ] Run Test 1: Rapid same-customer messages
- [ ] Run Test 2: Multiple customers concurrently
- [ ] Run Test 3: Stale lock recovery
- [ ] Monitor locks: `sqlite3 thread_store.db "SELECT * FROM processing_lock"`
- [ ] Check logs for "Acquired lock" and "Another worker processing"
- [ ] Verify no duplicate responses sent to customers

---

## üöÄ **Deployment**

```bash
# 1. Restart service to apply changes
sudo systemctl restart watibot4

# 2. Verify multiple workers started
ps aux | grep "uvicorn app.main:app" | wc -l
# Should show 5 (1 master + 4 workers)

# 3. Monitor startup
journalctl -u watibot4 -f | grep -E "(STARTUP|LOCK)"

# 4. Watch for lock activity
watch 'sqlite3 thread_store.db "SELECT * FROM processing_lock"'
```

---

**End of Documentation**
